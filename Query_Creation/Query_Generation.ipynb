{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3732,
     "status": "ok",
     "timestamp": 1747748379367,
     "user": {
      "displayName": "Compaltan",
      "userId": "14341232049321093230"
     },
     "user_tz": -120
    },
    "id": "FeSgKoqXdxEp",
    "outputId": "910b1fe8-08f8-4609-9e96-cbaa4e7ded91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85622,
     "status": "ok",
     "timestamp": 1747748251647,
     "user": {
      "displayName": "Compaltan",
      "userId": "14341232049321093230"
     },
     "user_tz": -120
    },
    "id": "gA8rIdiA32wE",
    "outputId": "114486c2-e360-4168-d94c-3b7167445a4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.3/489.3 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "pip install -q --upgrade bitsandbytes transformers accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-3Fv11Q8bkV"
   },
   "source": [
    "before loading torch, we run the cell below to enable dynamic memory segment expansion (that PyTorch can grow its memory blocks on the GPU as needed, instead of pre-allocating fixed chunks), which helps prevent out-of-memory (OOM) crashes due to memory fragmentation when using GPU-heavy models like LLaMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kStKndDiwLJp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZRFFZcIR3_6"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch, json, re, gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "login(token=\"\", add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLSYGuWY1f6f"
   },
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45SPaT4a80rl"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a precise JSON generator for mathematical QA datasets. \"\n",
    "    \"Your ONLY valid output is a JSON object with exactly one key \\\"questions\\\" \"\n",
    "    \"whose value is a list containing one and only one string. \"\n",
    "    \"NEVER add markdown fences, labels, explanations, or additional keys. \"\n",
    "    \"NEVER prefix your output with any words like “Question:” or “Sure! Here is”. \"\n",
    "    \"Your output MUST start with “{” and end with “}”. \"\n",
    "    \"ALWAYS verify your output is valid JSON before replying.\"\n",
    ")\n",
    "\n",
    "USER_TEMPLATE = (\n",
    "    \"Task:\\n\"\n",
    "    \"1. Read the theorem below.\\n\"\n",
    "    \"2. Write ONE question that could be answered by citing that theorem.\\n\"\n",
    "    \"   • Use clear, graduate-level mathematical language.\\n\"\n",
    "    \"   • Include LaTeX for any symbols or formulas.\\n\"\n",
    "    \"3. Return your answer as a JSON object with exactly one key \\\"questions\\\" \"\n",
    "    \"whose value is a list containing exactly one string.\\n\\n\"\n",
    "    \"Theorem:\\n\"\n",
    "    \"{THEOREM_TEXT}\\n\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_alt = (\n",
    "    \"You are a formatting guard.\\n\"\n",
    "    \"Your ONLY valid reply is a JSON object that:\\n\"\n",
    "    \"• has exactly one key named \\\"questions\\\"\\n\"\n",
    "    \"• whose value is a JSON list containing exactly one string\\n\\n\"\n",
    "    \"Never add any other keys, text, Markdown fences, or prefixes.\\n\"\n",
    "    \"Your reply MUST begin with “{” and end with “}”.\\n\"\n",
    "    \"Always verify the JSON is syntactically valid before sending.\"\n",
    ")\n",
    "\n",
    "USER_TEMPLATE_alt = (\n",
    "    \"Task:\\n\"\n",
    "    \"You are building a retrieval-augmented QA system for graduate-level mathematics.\\n\"\n",
    "    \"For the theorem given below, create ONE natural-language question that a student\\n\"\n",
    "    \"would ask if they needed to cite (or apply) this result while solving a problem.\\n\\n\"\n",
    "    \"Guidelines:\\n\"\n",
    "    \"•  Use precise, graduate-level wording; include LaTeX for symbols where helpful.\\n\"\n",
    "    \"•  Do NOT mention equation numbers, labels, “Theorem X”, “Lemma Y”, or citations.\\n\"\n",
    "    \"•  Do NOT add explanatory sentences, bullet points, or multiple questions.\\n\"\n",
    "    \"•  Output EXACTLY one JSON object with key \\\"questions\\\" whose value is a list\\n\"\n",
    "    \"  containing exactly ONE string — your question.\\n\\n\"\n",
    "    \"Return ONLY the JSON object. No extra prose.\\n\\n\"\n",
    "    \"Theorem:\\n\"\n",
    "    \"{THEOREM_TEXT}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJVJHPamv59d"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "05fa78cb0a98443aad1c80856a58ebc7",
      "9deace65b4994cf5941d2e9d887ac7b6",
      "b76ab888f0854b9992c6639f3a5a5396",
      "1713317ed98b40b2a26a2b06870a6823",
      "c478b371e9e74b24809bb9767a2f8fbd",
      "f69f9c3106fe4ae3bd9e726aadbc2aa1",
      "cad02e8b364d4dc2976952f9217e9e08",
      "9cfb00c5b77c49bfacb067010948313f",
      "6cec9a6e14584db7a7fe8b276567f583",
      "4b57ceafc41641f3badb13744d18dd00",
      "26f092f80ab04d59bb29f902da939e91",
      "a4f91ef0974c4d3aab3a14fe9bbac7e8",
      "01256959d4204aa694ad6563565f89ef",
      "01dc4924f2f24056ac8d12f0a532af90",
      "5d1ba4df4b1d439c9e9f6057c9b9f81e",
      "a61d8829dadd4d9eb7378fb048a67f45",
      "78300efa96a14359a60e8e33431ad497",
      "763aa4b4bdb24e91b385c7337cc13254",
      "0b16f6479a3e4513b73c18865796ff11",
      "08fbec7fc7334c58a92d4e2d2dbcd002",
      "9dc60374b0fa4c2b919f04b446bb5bf8",
      "88443a36ec0f454683c83a2457ae9c5f",
      "fff0824b66db422886b8c6d0614dc03a",
      "4965798a3b47410eaea6c4e7d1afe58b",
      "dbe97c0a959544a9b3e4042968404c92",
      "1ee716d6bc9047c7b62ee249a31de4ee",
      "651628f1d89747c4a7ec12dd03532ab0",
      "b21238a28a8c4454895aadb41f6afa13",
      "863e80b8a58a4fe8a250a7f067b68653",
      "19ad93bdb6b64750a70d1d2526ed07ca",
      "a7ba096b1c3e4315908c2e6e443f809b",
      "62830d1e720344759241874acca0c77c",
      "e191c802701844ee93f64c7e46bd68c8",
      "ffc868d75b974af6b0ab9e8508107419",
      "0436145453d84fea8c77ab478a3a6638",
      "3fd5c8afe3cd4a80811f9a6a70b71165",
      "3f012533869242848ec700aa82b2bade",
      "7f3e43d244ad4f378424dcc3e5097150",
      "271c895fb9294a258fabbf1b4f40fe5a",
      "2339a8a11fcb4f7b9f30ce1b1f9b475f",
      "406c64ff8d184541821a52a395d71e10",
      "aeaf5da7b5a44c759fe78b68e84267e2",
      "df7a2152d6524f06a763bb7d4e06a0b5",
      "9ecb8fc01cae4284a15ca09e1d66b16a",
      "69a4d8b45cbd4ae0854af1a8bf57b9f1",
      "586d81b8ee414bc785c132a7d868f3b8",
      "13bce83f6bb04ec58fb6847294e778c2",
      "151099b08b92451db5f5b4f0421b0ba7",
      "25a06b8ffac64705aa7de34b8f5fa509",
      "d1ccb64dab1743b39b45b87872b53ab1",
      "c5e2cb8ed8b649bba15fba0535ccb04a",
      "b3e30394f9c3437cbdfe0c9eef770a4c",
      "7bf347d995924c8598f1d6208e7c9f27",
      "13db93cc091b47b291ff365d84289ffc",
      "dc2bd709910e4fd7bf2a24b8f382abdc",
      "ec37de5ae58f4ac5a5db4e3d2d5953c6",
      "7b928b1c31b64574967f2c5cfe60f32d",
      "cfc36161e7e347cb801a8596d55460ba",
      "d0999f87ba8948fd9ac777c5abf33739",
      "2668f8928d704679b87c6f5c6c71be01",
      "fb5575ca514d4b67839a4e7d26684148",
      "c046d2703ae8445fa541c451c87db82c",
      "d80cdebafb0c404eb23cfb4c686b5e4d",
      "10208663f1fd416ea3424930b953da85",
      "f5d8c708723d4f718342b66b1fe34f7e",
      "16ed4704562346d09fd87eab4b6d7331",
      "68bb8b5cb85347fb87023194fe7b48ad",
      "080780fe5c9844c3b7b3ee72d27c99b0",
      "6ea2c474d9ec4a21a7e8ecfe12e6d029",
      "85fad10a986d48768ace00d56c51dbbd",
      "8d9439de20e442b98a9a9612a48da4b1",
      "7b4b50d1dc404b9e8018fe75caa7214e",
      "03692f67dff34cd6b0561bc8c239b78c",
      "a6a47d9c8f4f495daa708edff8569e6a",
      "fe48f9cc48424a299cb5cfbe611b134d",
      "d58a7c9a4a024088afb68902a605e97d",
      "e03928de7486475b8694091542585a7b",
      "13e913078ff2468d91ba6990e0d355b8",
      "807de26f50e647f4b9ecae71aecfe533",
      "cb79bfbcbf3f4f309c4a3612a0a8c83a",
      "4de6a68e5a5644b8b054747cc011e9bc",
      "9b5709e5356042e88311c1aec7247ef8",
      "240907917f04462ba003f498dea253b8",
      "8ed698c3a96a42028b701772852357bb",
      "421e3caabc764fe7b1aba7aa1a8214fa",
      "a35bfe2d33c7470ea7d68b578dbb2668",
      "370ecf5e739d46218e135f5b82f9185b",
      "d20aeb3714b34f13a7bf5be040147e3c",
      "304ee5c19ef545c3956dc4bdabe5c333",
      "b1e44a793c784b0aa09a2b821cee9e48",
      "f0b5c664e8ff4b5693bb01228949a41b",
      "7bab1e490519459ea7156d5f31e1dee5",
      "876a8247336549a78afea762a706934e",
      "b92cd839e7d84b7fbcc3c38eed60f4e6",
      "08682b0f334547879a693ded44f5ca98",
      "3da10c36828c47f5a9a2366bb661d8aa",
      "183c254716394cea99b4a6a2143c0828",
      "8b43d7d5946f4f87b418edd8abcc828b",
      "24949cd89bac41d7820b92a1ed4165be",
      "6ba40d8009204247802b18e1932ecb34",
      "9dab50cf91b6430bbd91d5a75fe67276",
      "d71cf890e08e4e9f878d541d0dd2915d",
      "be70cc2d2de24babaf6186ffe5728621",
      "86c9c0f2f035448791f2dff7fd57f422",
      "94ef933e70314522be743caf5750e8bd",
      "fb24fa107d3d4e1fa797168cea403c96",
      "06cc24f291184473b1c5cd348c765e3f",
      "ddf093f2590947e1b6749455126b0a4e",
      "da5fbd68f4b34ca3a7c82bba8cf0474f",
      "3b0abc88fe2c4691b3018182942b9693"
     ]
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1747748375683,
     "user": {
      "displayName": "Compaltan",
      "userId": "14341232049321093230"
     },
     "user_tz": -120
    },
    "id": "w5rNhHXG6dp0",
    "outputId": "ab74022a-7244-4e58-8a81-d1374bc27c06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fa78cb0a98443aad1c80856a58ebc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f91ef0974c4d3aab3a14fe9bbac7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff0824b66db422886b8c6d0614dc03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc868d75b974af6b0ab9e8508107419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a4d8b45cbd4ae0854af1a8bf57b9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec37de5ae58f4ac5a5db4e3d2d5953c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bb8b5cb85347fb87023194fe7b48ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e913078ff2468d91ba6990e0d355b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304ee5c19ef545c3956dc4bdabe5c333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba40d8009204247802b18e1932ecb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Load tokenizer & model\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# 1) Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\" ###\n",
    "\n",
    "# 2) 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 3) Model loading\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True         # offloads some weights to CPU if needed\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBjq3KX_v8lw"
   },
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSsdFxEq1OqK"
   },
   "outputs": [],
   "source": [
    "def generate_questions_batch(df, system_prompt, user_template,\n",
    "                             tokenizer, model,\n",
    "                             batch_size=4, max_new_tokens=120):\n",
    "    \"\"\"\n",
    "    df: DataFrame with column 'content'\n",
    "    tokenizer: HuggingFace AutoTokenizer\n",
    "    model:     HuggingFace AutoModelForCausalLM (already on GPU)\n",
    "    \"\"\"\n",
    "    prompts = [\n",
    "        f\"<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n\"\n",
    "        f\"[INST]\\n{user_template.format(THEOREM_TEXT=theo)}[/INST]\"\n",
    "        for theo in df[\"content\"]\n",
    "    ]\n",
    "\n",
    "    questions = []\n",
    "    device = model.device\n",
    "    total_batches = (len(prompts) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size),\n",
    "                  desc=\"Generating questions\",\n",
    "                  total=total_batches,\n",
    "                  unit=\"batch\"):\n",
    "        batch_prompts = prompts[i : i + batch_size]\n",
    "\n",
    "        # 1) Tokenize the whole batch in one go\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        # 2) Generate in one forward pass\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                top_p=1.0,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # 1. real prompt length per row  (tensor shape: [batch])\n",
    "        prompt_lens = inputs[\"attention_mask\"].sum(dim=1)\n",
    "\n",
    "        # 2. cut away the prompt for every row without a Python loop\n",
    "        gen_ids = [\n",
    "            seq[p_len:]            # keep only the new tokens\n",
    "            for seq, p_len in zip(output_ids, prompt_lens.tolist())\n",
    "        ]\n",
    "        # 3b) Decode only those new tokens\n",
    "        batch_texts = tokenizer.batch_decode(\n",
    "            gen_ids,\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # 4) Parse each output exactly like your single-call version\n",
    "        for out_text in batch_texts:\n",
    "            # JSON‐block + backslash‐escape\n",
    "            #print(f\"\\n out text: {out_text}\\n\")\n",
    "            match = re.search(\n",
    "                r'\\{[^{}]*\"questions\"\\s*:\\s*\\[\\s*\".+?\"\\s*\\][^{}]*\\}',\n",
    "                out_text,\n",
    "                flags=re.DOTALL\n",
    "            )\n",
    "            if match:\n",
    "                js_blob = match.group(0).replace(\"\\\\\", \"\\\\\\\\\")\n",
    "                #print(f\"\\n\\n Match: {js_blob}\\n\\n\")\n",
    "                try:\n",
    "                    js = json.loads(js_blob)\n",
    "                    questions.append(js[\"questions\"][0])\n",
    "                    continue\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Fallback\n",
    "            for line in out_text.splitlines():\n",
    "                clean = line.strip()\n",
    "                if clean and not clean.lower().startswith(\"question\"):\n",
    "                    questions.append(clean)\n",
    "                    break\n",
    "            else:\n",
    "                questions.append(\"__FAILED__\")\n",
    "\n",
    "    df[\"question\"] = questions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04nMduK_wAlu"
   },
   "source": [
    "## Single Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NbWp2MN1rO1"
   },
   "outputs": [],
   "source": [
    "INPUT_CSV  = \"/content/drive/Shareddrives/Master_Thesis/Query_Creation/Data/theorems_and_lemmas_200.csv\"\n",
    "INPUT_CSV = \"/content/drive/Shareddrives/Master_Thesis/Query_Creation/Data/failed_passages_I.csv\"\n",
    "\n",
    "INPUT_CSV = \"/content/drive/Shareddrives/Master_Thesis/Query_Creation/Data/passages_200.csv\"\n",
    "OUTPUT_CSV = \"/content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_v7.csv\"\n",
    "\n",
    "#  Load data\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "#  Run\n",
    "df = generate_questions_batch(\n",
    "    df,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    user_template=USER_TEMPLATE,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    batch_size=32,\n",
    "    max_new_tokens=100\n",
    ")\n",
    "\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Saved to {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1747664493828,
     "user": {
      "displayName": "Compaltan",
      "userId": "14341232049321093230"
     },
     "user_tz": -120
    },
    "id": "doaTO_sS6xku",
    "outputId": "2c21292f-00db-4dc9-c663-e0855b441270"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean up cache for less gpu memory usage\n",
    "torch.cuda.synchronize()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlPBrrqgJzKV"
   },
   "source": [
    "# Generator for Big Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPA7jdy_J8on"
   },
   "outputs": [],
   "source": [
    "def run_generation_with_checkpoints(\n",
    "    df_full,\n",
    "    chunk_size,\n",
    "    checkpoint_path,\n",
    "    already_done=0,\n",
    "    **gen_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes the full DataFrame in chunks and saves to disk after each chunk.\n",
    "    Resumes from existing checkpoint if present.\n",
    "\n",
    "    After each chunk, synchronizes GPU, empties cache, and runs GC to avoid fragmentation/OOM.\n",
    "    \"\"\"\n",
    "    total_rows = len(df_full)\n",
    "\n",
    "    # Load existing checkpoint if present\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        processed_df = pd.read_csv(checkpoint_path)\n",
    "        already_done = len(processed_df)\n",
    "        print(f\"Resuming from checkpoint. Already processed {already_done} rows.\")\n",
    "    else:\n",
    "        processed_df = pd.DataFrame()  # empty\n",
    "\n",
    "    # Iterate over chunks\n",
    "    for start in range(already_done, total_rows, chunk_size):\n",
    "        end = min(start + chunk_size, total_rows)\n",
    "        chunk_df = df_full.iloc[start:end].copy()\n",
    "        chunk_df.reset_index(drop=True, inplace=True)\n",
    "        chunk_df['id'] = chunk_df.index\n",
    "\n",
    "        print(f\"\\nProcessing rows {start} to {end - 1}\")\n",
    "        # Run generation on the chunk\n",
    "        chunk_with_qs = generate_questions_batch(chunk_df, **gen_kwargs)\n",
    "\n",
    "        # Append results\n",
    "        processed_df = pd.concat([processed_df, chunk_with_qs], ignore_index=True)\n",
    "\n",
    "        # Save checkpoint without writing the index column\n",
    "        processed_df.to_csv(checkpoint_path, index=False)\n",
    "        print(f\"Saved checkpoint with {len(processed_df)} rows to {checkpoint_path}\")\n",
    "\n",
    "        # Clear GPU cache and Python garbage to avoid fragmentation\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"Cleared CUDA cache and ran garbage collection.\")\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1304
    },
    "executionInfo": {
     "elapsed": 3493,
     "status": "ok",
     "timestamp": 1747748382866,
     "user": {
      "displayName": "Compaltan",
      "userId": "14341232049321093230"
     },
     "user_tz": -120
    },
    "id": "oUSePqHE6Qf3",
    "outputId": "3fe83a67-228f-491a-89d6-cfab110a4a27"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 16667,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4811,\n        \"min\": 0,\n        \"max\": 16666,\n        \"num_unique_values\": 16667,\n        \"samples\": [\n          13604,\n          14175,\n          10140\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01460715096140373,\n        \"min\": 2501.00724,\n        \"max\": 2501.05889,\n        \"num_unique_values\": 768,\n        \"samples\": [\n          2501.05123,\n          2501.02924,\n          2501.04766\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 768,\n        \"samples\": [\n          \"d-antimagic labelings of oriented 2-regular graphs\",\n          \"yamada-watanabe uniqueness results for spdes driven by wiener and pure   jump processes\",\n          \"decoding rank metric reed-muller codes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 277,\n        \"samples\": [\n          \"math.dg math.ap\",\n          \"stat.ml cs.ai cs.cl cs.cr cs.lg math.st stat.th\",\n          \"math.ag math.rt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"lemmas\",\n          \"definitions\",\n          \"propositions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16575,\n        \"samples\": [\n          \"%\\\\label{T-main02}\\r\\nFor the weak almost ${\\\\cal S}$-structure\\r\\n%$(f,Q,\\\\xi_i,\\\\eta^i,g)$\\r\\non a closed manifold $M^{2n+s}$ with conditions\\r\\n%\\\\eqref{Eq-2assump},\\r\\n\\\\begin{equation*}%\\\\label{Eq-2assump}\\r\\n (a)~\\\\pounds_{\\\\xi_i}\\\\,\\\\widetilde{Q} =0,\\\\qquad\\r\\n (b)~N^{(5)}(\\\\xi_i,\\\\,\\\\cdot\\\\,, \\\\,\\\\cdot) =0,\\r\\n\\\\end{equation*}\\r\\n(satisfied by weak $f$-K-manifolds)\\r\\nthe following integral formula is true:\\r\\n\\\\begin{eqnarray}\\\\label{E-k-PW}\\r\\n \\\\int_M \\\\Big\\\\{\\\\sum\\\\nolimits_{\\\\,i} \\\\big({\\\\rm Ric}(\\\\xi_i,\\\\xi_i)  + \\\\|Q^{-1} f h_i\\\\|^2\\r\\n - (\\\\tr Q^{-1} f h_i)^2\\\\,\\\\big) - s\\\\,\\\\|f\\\\|^2 \\\\Big\\\\}\\\\,d\\\\,{\\\\rm vol} = 0.\\r\\n\\\\end{eqnarray}\",\n          \"\\\\label{r1}\\r\\nIf $F$ is a fundamental basic block obtained from $\\\\CF(n)$ by removal of $N-l$ doubly irreducible elements such that $\\\\Red(F) = \\\\Red(\\\\CF(n))$ where $n>2$, then \\r\\n$F \\\\in \\\\mathscr{F}_n(l)$, where $N = \\\\eta(\\\\CF(n)) = \\\\binom{n}{2}$ and $\\\\lfloor \\\\frac{n+1}{2}\\\\rfloor \\\\leq l \\\\leq N$.\\r\\nFurther, if $l \\\\geq N-n+2$ then $f(n, l) = |\\\\mathscr{F}_n(l)| = \\\\binom{N}{l}$.\",\n          \"\\\\label{lemma:lyapunov-collapse}\\nLet $(M, g, \\\\lambda)$ be a thermostat without conjugate points, and let $\\\\nu$ be a Borel ergodic measure on $SM$. We have $G_s^*(v) = G_u^*(v)$ $\\\\nu$-almost everywhere if and only if $\\\\chi^u(v) = \\\\chi^s(v) = 0$ $\\\\nu$-almost everywhere. Furthermore, if $G_s^*(v) = G_u^*(v)$ for all $v \\\\in SM$, then the topological entropy of the thermostat flow is zero.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-6b3464be-415c-4dbe-97fd-5f1fa3b33fff\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paper id</th>\n",
       "      <th>title</th>\n",
       "      <th>categories</th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2501.00724</td>\n",
       "      <td>category o for quantum loop algebras</td>\n",
       "      <td>math.rt math.qa</td>\n",
       "      <td>theorems</td>\n",
       "      <td>\\label{thm:main}\\n\\t\\nConsider any Kac-Moody L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2501.00724</td>\n",
       "      <td>category o for quantum loop algebras</td>\n",
       "      <td>math.rt math.qa</td>\n",
       "      <td>theorems</td>\n",
       "      <td>\\label{thm:my refined}\\n\\nLet $\\fg$ be of fini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2501.00724</td>\n",
       "      <td>category o for quantum loop algebras</td>\n",
       "      <td>math.rt math.qa</td>\n",
       "      <td>theorems</td>\n",
       "      <td>\\label{thm:toroidal}\\n\\nFor a polynomial $\\ell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2501.00724</td>\n",
       "      <td>category o for quantum loop algebras</td>\n",
       "      <td>math.rt math.qa</td>\n",
       "      <td>theorems</td>\n",
       "      <td>\\label{thm:simple}\\n\\n(\\cite{HJ}) Up to isomor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2501.00724</td>\n",
       "      <td>category o for quantum loop algebras</td>\n",
       "      <td>math.rt math.qa</td>\n",
       "      <td>theorems</td>\n",
       "      <td>\\label{thm:quantum to shuffle}\\n\\nWe have $\\em...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b3464be-415c-4dbe-97fd-5f1fa3b33fff')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6b3464be-415c-4dbe-97fd-5f1fa3b33fff button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6b3464be-415c-4dbe-97fd-5f1fa3b33fff');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-488d2e7f-ff34-4952-974f-86e49702ab41\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-488d2e7f-ff34-4952-974f-86e49702ab41')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-488d2e7f-ff34-4952-974f-86e49702ab41 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   id    paper id                                 title       categories  \\\n",
       "0   0  2501.00724  category o for quantum loop algebras  math.rt math.qa   \n",
       "1   1  2501.00724  category o for quantum loop algebras  math.rt math.qa   \n",
       "2   2  2501.00724  category o for quantum loop algebras  math.rt math.qa   \n",
       "3   3  2501.00724  category o for quantum loop algebras  math.rt math.qa   \n",
       "4   4  2501.00724  category o for quantum loop algebras  math.rt math.qa   \n",
       "\n",
       "       type                                            content  \n",
       "0  theorems  \\label{thm:main}\\n\\t\\nConsider any Kac-Moody L...  \n",
       "1  theorems  \\label{thm:my refined}\\n\\nLet $\\fg$ be of fini...  \n",
       "2  theorems  \\label{thm:toroidal}\\n\\nFor a polynomial $\\ell...  \n",
       "3  theorems  \\label{thm:simple}\\n\\n(\\cite{HJ}) Up to isomor...  \n",
       "4  theorems  \\label{thm:quantum to shuffle}\\n\\nWe have $\\em...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/content/drive/Shareddrives/Master_Thesis/Query_Creation/Data/df_100_chunk_1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11258670,
     "status": "ok",
     "timestamp": 1747698085533,
     "user": {
      "displayName": "Compaltan",
      "userId": "14341232049321093230"
     },
     "user_tz": -120
    },
    "id": "-aC3qXdSKDyV",
    "outputId": "aad1d6ec-5c2e-4324-e04c-ff177d616963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint. Already processed 6000 rows.\n",
      "\n",
      "Processing rows 6000 to 6999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:27<13:59, 27.08s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [00:51<12:38, 25.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:12<11:17, 23.37s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [01:35<10:58, 23.51s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:02<11:01, 24.51s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [02:30<11:09, 25.76s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [03:07<12:18, 29.56s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [03:52<13:44, 34.37s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [04:30<13:36, 35.48s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:05<12:55, 35.26s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [05:48<13:15, 37.89s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:21<12:05, 36.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [07:02<11:54, 37.59s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:45<11:46, 39.27s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [08:24<11:04, 39.11s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [08:51<09:30, 35.64s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [09:22<08:33, 34.25s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [10:03<08:25, 36.08s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [10:39<07:49, 36.13s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [11:19<07:28, 37.40s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:54<06:42, 36.57s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [12:24<05:47, 34.77s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:49<04:44, 31.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [13:12<03:52, 29.09s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [13:51<03:45, 32.20s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [14:27<03:19, 33.26s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [14:59<02:44, 32.93s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [15:39<02:19, 34.96s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [16:14<01:45, 35.10s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [16:38<01:03, 31.68s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [17:04<00:29, 29.88s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [17:21<00:00, 32.54s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 7000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_1.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 7000 to 7999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:35<18:24, 35.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [01:09<17:10, 34.33s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:36<15:08, 31.34s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:12<15:27, 33.13s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:39<13:54, 30.91s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:18<14:31, 33.52s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [03:48<13:31, 32.46s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:18<12:42, 31.77s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [04:48<11:58, 31.23s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:32<12:48, 34.91s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [06:17<13:17, 38.00s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:51<12:15, 36.79s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [07:23<11:14, 35.50s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:53<10:09, 33.86s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [08:24<09:19, 32.89s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [08:50<08:16, 31.02s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [09:20<07:40, 30.71s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [09:46<06:47, 29.09s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [10:14<06:15, 28.92s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [10:49<06:07, 30.60s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:21<05:42, 31.10s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [11:55<05:20, 32.03s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:26<04:44, 31.58s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [13:00<04:17, 32.24s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [13:26<03:33, 30.56s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [13:58<03:04, 30.81s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [14:43<02:56, 35.24s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [15:12<02:12, 33.22s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [15:43<01:37, 32.59s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [16:23<01:09, 34.75s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [16:51<00:32, 32.70s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [17:02<00:00, 31.96s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 8000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_1.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 8000 to 8999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:31<16:19, 31.59s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [01:08<17:21, 34.72s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:43<16:55, 35.03s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:11<14:53, 31.92s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:46<14:56, 33.21s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:12<13:21, 30.83s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [03:48<13:31, 32.48s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:20<12:52, 32.20s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [04:47<11:44, 30.62s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:16<11:03, 30.14s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [05:48<10:43, 30.65s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:15<09:54, 29.73s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [06:48<09:44, 30.75s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:17<09:01, 30.07s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [07:43<08:12, 28.94s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [08:17<08:07, 30.49s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [08:56<08:15, 33.03s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [09:22<07:09, 30.67s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [09:59<07:05, 32.72s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [10:44<07:15, 36.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:25<06:56, 37.84s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [12:00<06:09, 37.00s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:40<05:40, 37.83s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [13:17<04:59, 37.49s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [13:57<04:29, 38.46s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [14:34<03:48, 38.03s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [15:10<03:05, 37.19s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [15:42<02:23, 35.79s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [16:09<01:39, 33.24s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [16:40<01:05, 32.56s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [17:15<00:33, 33.05s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [17:27<00:00, 32.73s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 9000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_1.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 9000 to 9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:39<20:24, 39.50s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [01:18<19:44, 39.48s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [02:03<20:05, 41.57s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:32<17:10, 36.80s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [03:00<15:06, 33.57s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:29<13:52, 32.01s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [03:56<12:40, 30.44s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:29<12:28, 31.18s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [05:01<12:04, 31.52s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:33<11:35, 31.61s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [05:59<10:28, 29.91s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:45<11:34, 34.74s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [07:22<11:12, 35.39s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:56<10:29, 34.96s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [08:19<08:53, 31.37s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [08:54<08:42, 32.65s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [09:28<08:14, 32.96s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [10:03<07:50, 33.62s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [10:36<07:15, 33.50s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [11:08<06:36, 33.07s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:47<06:22, 34.79s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [12:11<05:15, 31.56s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:39<04:32, 30.29s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [13:10<04:06, 30.77s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [13:49<03:52, 33.20s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [14:21<03:16, 32.68s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [14:52<02:40, 32.17s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [15:53<02:43, 40.82s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [16:32<02:00, 40.21s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [17:01<01:13, 36.86s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [17:29<00:34, 34.37s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [17:41<00:00, 33.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 10000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_1.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 10000 to 10999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:25<13:15, 25.67s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [01:08<17:44, 35.48s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:41<16:44, 34.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:11<15:17, 32.78s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:44<14:43, 32.71s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:13<13:39, 31.52s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [03:48<13:37, 32.72s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:26<13:45, 34.38s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [04:49<11:51, 30.91s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:14<10:36, 28.95s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [05:48<10:42, 30.60s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:25<10:47, 32.37s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [06:51<09:39, 30.51s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:29<09:48, 32.72s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [08:16<10:33, 37.24s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [08:55<10:02, 37.65s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [09:35<09:35, 38.38s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [10:15<09:05, 38.94s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [10:42<07:37, 35.19s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [11:21<07:16, 36.39s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:54<06:27, 35.26s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [12:21<05:29, 32.97s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:56<05:00, 33.39s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [13:37<04:45, 35.71s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [14:15<04:14, 36.41s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [14:44<03:25, 34.29s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [15:23<02:59, 35.81s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [16:04<02:28, 37.17s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [16:44<01:54, 38.20s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [17:20<01:14, 37.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [17:59<00:38, 38.04s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [18:15<00:00, 34.24s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 11000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_1.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 11000 to 11999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:46<24:06, 46.66s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [01:13<17:32, 35.10s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:51<17:27, 36.14s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:29<17:13, 36.92s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [03:05<16:27, 36.58s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:33<14:34, 33.64s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [04:00<13:10, 31.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:27<12:01, 30.04s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [05:05<12:30, 32.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:38<12:01, 32.79s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [06:05<10:50, 30.98s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:42<10:56, 32.81s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [07:22<11:03, 34.94s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [08:04<11:09, 37.22s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [08:32<09:45, 34.44s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [09:06<09:07, 34.19s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [09:35<08:09, 32.64s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [10:01<07:08, 30.60s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [10:32<06:39, 30.70s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [10:59<05:57, 29.75s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:36<05:50, 31.91s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [12:11<05:26, 32.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:50<05:13, 34.78s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [13:29<04:47, 35.95s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [14:05<04:11, 35.89s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [14:34<03:23, 33.92s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [15:13<02:57, 35.46s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [15:49<02:21, 35.41s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [16:29<01:51, 37.01s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [17:03<01:11, 35.93s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [17:35<00:34, 34.73s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [17:48<00:00, 33.40s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 12000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_1.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 12000 to 12999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:34<18:00, 34.86s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [01:00<14:50, 29.68s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:29<14:12, 29.39s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:00<13:58, 29.93s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:38<14:48, 32.91s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:21<15:36, 36.03s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [03:47<13:40, 32.82s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:22<13:30, 33.75s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [05:02<13:41, 35.71s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:34<12:39, 34.52s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [06:01<11:11, 32.00s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:32<10:34, 31.72s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [07:15<11:10, 35.29s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:40<09:35, 32.00s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [08:12<09:05, 32.09s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [08:40<08:13, 30.85s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [09:07<07:28, 29.87s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [09:48<07:43, 33.09s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [10:32<07:52, 36.35s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [11:07<07:12, 36.02s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:49<06:56, 37.86s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [12:26<06:14, 37.42s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:59<05:24, 36.10s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [13:25<04:24, 33.06s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [13:57<03:49, 32.83s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [14:27<03:11, 31.96s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [15:00<02:40, 32.18s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [15:31<02:07, 31.97s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [16:13<01:45, 35.06s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [17:00<01:16, 38.37s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [17:38<00:38, 38.30s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [17:52<00:00, 33.50s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 13000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_1.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 13000 to 13999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:36<18:52, 36.55s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [01:09<17:10, 34.36s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:38<15:23, 31.86s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:14<15:37, 33.50s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:55<16:17, 36.22s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:31<15:42, 36.24s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [04:15<16:07, 38.71s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:53<15:23, 38.48s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [05:17<13:05, 34.14s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:54<12:49, 34.99s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [06:25<11:44, 33.54s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:55<10:52, 32.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [07:23<09:50, 31.08s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [08:02<10:06, 33.71s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [08:36<09:32, 33.65s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [09:02<08:20, 31.30s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [09:31<07:41, 30.75s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [10:11<07:47, 33.43s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [10:43<07:10, 33.15s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [11:12<06:20, 31.67s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:36<05:24, 29.46s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [12:14<05:20, 32.10s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:58<05:20, 35.65s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [13:28<04:30, 33.81s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [14:14<04:21, 37.42s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [14:51<03:45, 37.51s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [15:17<02:50, 34.10s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [15:47<02:11, 32.76s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [16:16<01:35, 31.70s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [17:00<01:10, 35.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [17:32<00:34, 34.34s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [17:45<00:00, 33.31s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 14000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_1.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 14000 to 14999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:29<15:18, 29.64s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [01:10<18:07, 36.26s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:41<16:24, 33.95s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:12<15:13, 32.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:55<16:19, 36.29s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:33<15:58, 36.85s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [04:07<14:57, 35.92s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:32<12:59, 32.48s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [05:04<12:28, 32.54s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:44<12:43, 34.70s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [06:14<11:38, 33.27s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:47<11:03, 33.17s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [07:30<11:27, 36.19s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [08:02<10:26, 34.82s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [08:30<09:17, 32.81s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [09:05<08:57, 33.60s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [09:31<07:48, 31.23s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [09:52<06:35, 28.26s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [10:31<06:47, 31.36s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [11:00<06:08, 30.70s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:41<06:12, 33.90s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [12:26<06:12, 37.24s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [13:00<05:25, 36.14s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [13:30<04:34, 34.34s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [14:08<04:07, 35.31s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [14:39<03:25, 34.21s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [15:16<02:54, 34.82s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [15:55<02:24, 36.06s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [16:27<01:45, 35.01s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [17:03<01:10, 35.40s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [17:45<00:37, 37.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [18:00<00:00, 33.76s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 15000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_1.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 15000 to 15999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:31<16:02, 31.06s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [01:06<16:53, 33.80s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:40<16:15, 33.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:14<15:53, 34.04s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:49<15:28, 34.40s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:19<14:15, 32.92s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [03:49<13:13, 31.74s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:17<12:13, 30.57s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [04:45<11:23, 29.71s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:17<11:10, 30.46s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [05:48<10:41, 30.54s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:26<10:57, 32.86s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [06:59<10:27, 33.04s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:26<09:23, 31.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [07:50<08:11, 28.91s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [08:22<07:57, 29.84s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [08:47<07:05, 28.38s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [09:22<07:07, 30.52s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [09:54<06:41, 30.89s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [10:27<06:17, 31.46s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [10:52<05:24, 29.52s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [11:27<05:12, 31.29s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:00<04:44, 31.62s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [12:24<03:55, 29.47s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [12:56<03:32, 30.36s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [13:28<03:03, 30.59s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [14:02<02:39, 31.85s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [14:30<02:01, 30.44s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [14:59<01:30, 30.19s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [15:38<01:05, 32.73s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [16:13<00:33, 33.56s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [16:30<00:00, 30.95s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 16000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_1.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 16000 to 16666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/21 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   5%|▍         | 1/21 [00:26<08:59, 26.96s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  10%|▉         | 2/21 [01:01<10:02, 31.71s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  14%|█▍        | 3/21 [01:37<10:03, 33.53s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 4/21 [02:13<09:44, 34.36s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  24%|██▍       | 5/21 [02:55<09:57, 37.36s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  29%|██▊       | 6/21 [03:35<09:32, 38.14s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  33%|███▎      | 7/21 [04:09<08:34, 36.74s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 8/21 [04:37<07:22, 34.01s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  43%|████▎     | 9/21 [05:16<07:04, 35.39s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  48%|████▊     | 10/21 [05:50<06:24, 34.97s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  52%|█████▏    | 11/21 [06:13<05:15, 31.52s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  57%|█████▋    | 12/21 [06:39<04:26, 29.64s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▏   | 13/21 [07:22<04:30, 33.80s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  67%|██████▋   | 14/21 [07:48<03:40, 31.55s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  71%|███████▏  | 15/21 [08:19<03:08, 31.36s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  76%|███████▌  | 16/21 [08:55<02:43, 32.61s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████  | 17/21 [09:37<02:21, 35.48s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  86%|████████▌ | 18/21 [10:05<01:39, 33.23s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  90%|█████████ | 19/21 [10:31<01:02, 31.17s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  95%|█████████▌| 20/21 [11:02<00:31, 31.02s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 21/21 [11:37<00:00, 33.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 16667 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_1.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/content/drive/Shareddrives/Master_Thesis/Query_Creation/Data/df_100_chunk_1.csv\")\n",
    "\n",
    "checkpoint_path=\"/content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_1.csv\"\n",
    "\n",
    "final_df = run_generation_with_checkpoints(\n",
    "    df_full=df,\n",
    "    chunk_size=1000,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    user_template=USER_TEMPLATE,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    batch_size=32,\n",
    "    max_new_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2859235,
     "status": "ok",
     "timestamp": 1747733723471,
     "user": {
      "displayName": "Compaltan",
      "userId": "14341232049321093230"
     },
     "user_tz": -120
    },
    "id": "rOeSM_nMhxve",
    "outputId": "ac5c8377-1694-458a-8c28-e993689e4030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint. Already processed 14000 rows.\n",
      "\n",
      "Processing rows 14000 to 14999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:33<17:04, 33.06s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [01:05<16:17, 32.59s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:41<16:32, 34.24s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:24<17:32, 37.58s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [03:03<17:15, 38.36s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:33<15:16, 35.26s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [04:06<14:23, 34.53s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:29<12:20, 30.83s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [05:04<12:19, 32.13s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:38<12:04, 32.91s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [06:08<11:11, 31.95s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:39<10:34, 31.71s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [07:17<10:37, 33.55s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:55<10:26, 34.80s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [08:28<09:42, 34.25s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [09:07<09:33, 35.83s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [09:32<08:08, 32.54s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [09:57<07:04, 30.29s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [10:30<06:42, 30.97s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [11:07<06:34, 32.84s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:41<06:05, 33.23s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [12:18<05:43, 34.34s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:53<05:12, 34.70s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [13:30<04:41, 35.16s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [13:57<03:48, 32.68s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [14:28<03:13, 32.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [14:58<02:38, 31.68s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [15:25<02:00, 30.08s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [15:56<01:31, 30.60s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [16:30<01:02, 31.44s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [17:03<00:32, 32.04s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [17:11<00:00, 32.25s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 15000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_2.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 15000 to 15999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:40<20:48, 40.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [01:20<20:05, 40.19s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [02:02<19:44, 40.84s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:31<17:00, 36.46s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [03:07<16:18, 36.23s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:42<15:31, 35.85s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [04:12<14:06, 33.84s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:46<13:34, 33.93s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [05:18<12:44, 33.23s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:58<13:01, 35.53s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [06:25<11:26, 32.67s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:53<10:27, 31.39s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [07:21<09:34, 30.25s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:47<08:45, 29.17s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [08:13<07:56, 28.03s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [08:42<07:32, 28.26s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [09:10<07:04, 28.32s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [09:45<07:04, 30.30s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [10:20<06:52, 31.71s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [10:58<06:44, 33.73s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:38<06:29, 35.42s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [12:12<05:51, 35.17s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:43<05:04, 33.85s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [13:14<04:23, 32.94s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [13:47<03:50, 32.92s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [14:26<03:28, 34.76s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [15:05<03:00, 36.01s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [15:44<02:27, 36.91s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [16:30<01:59, 39.78s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [16:56<01:10, 35.48s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [17:25<00:33, 33.66s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [17:38<00:00, 33.07s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 16000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_2.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 16000 to 16666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/21 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   5%|▍         | 1/21 [00:33<11:13, 33.67s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  10%|▉         | 2/21 [01:00<09:22, 29.58s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  14%|█▍        | 3/21 [01:30<08:52, 29.61s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 4/21 [02:06<09:12, 32.51s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  24%|██▍       | 5/21 [02:38<08:36, 32.31s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  29%|██▊       | 6/21 [03:11<08:06, 32.41s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  33%|███▎      | 7/21 [03:47<07:51, 33.70s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 8/21 [04:22<07:21, 33.96s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  43%|████▎     | 9/21 [04:56<06:48, 34.07s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  48%|████▊     | 10/21 [05:33<06:25, 35.03s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  52%|█████▏    | 11/21 [06:17<06:17, 37.74s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  57%|█████▋    | 12/21 [06:49<05:22, 35.80s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▏   | 13/21 [07:33<05:06, 38.32s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  67%|██████▋   | 14/21 [08:17<04:40, 40.12s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  71%|███████▏  | 15/21 [08:51<03:50, 38.38s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  76%|███████▌  | 16/21 [09:30<03:11, 38.35s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████  | 17/21 [10:14<02:41, 40.29s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  86%|████████▌ | 18/21 [10:42<01:48, 36.31s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  90%|█████████ | 19/21 [11:16<01:11, 35.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  95%|█████████▌| 20/21 [12:02<00:38, 38.81s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 21/21 [12:40<00:00, 36.23s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 16667 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_2.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/content/drive/Shareddrives/Master_Thesis/Query_Creation/Data/df_100_chunk_2.csv\")\n",
    "\n",
    "checkpoint_path=\"/content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_2.csv\"\n",
    "\n",
    "final_df = run_generation_with_checkpoints(\n",
    "    df_full=df,\n",
    "    chunk_size=1000,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    user_template=USER_TEMPLATE,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    batch_size=32,\n",
    "    max_new_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6804196,
     "status": "ok",
     "timestamp": 1747755275880,
     "user": {
      "displayName": "Compaltan",
      "userId": "14341232049321093230"
     },
     "user_tz": -120
    },
    "id": "nm3K-8y9iNhF",
    "outputId": "04be1638-e39b-4357-bf77-93fe11f848cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint. Already processed 10000 rows.\n",
      "\n",
      "Processing rows 10000 to 10999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:22<11:49, 22.88s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [00:48<12:11, 24.37s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:12<11:43, 24.27s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [01:51<14:07, 30.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:25<14:06, 31.34s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:05<14:55, 34.45s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [03:42<14:41, 35.26s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:18<14:12, 35.53s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [04:57<14:01, 36.61s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:34<13:23, 36.53s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [06:10<12:47, 36.54s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:41<11:38, 34.92s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [07:01<09:38, 30.46s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:24<08:23, 27.96s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [07:59<08:34, 30.29s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [08:33<08:20, 31.26s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [09:08<08:05, 32.35s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [09:35<07:11, 30.84s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [10:03<06:29, 29.96s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [10:36<06:11, 30.95s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:09<05:45, 31.39s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [11:30<04:43, 28.39s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:06<04:37, 30.80s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [12:33<03:56, 29.52s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [13:04<03:29, 29.93s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [13:40<03:10, 31.76s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [14:13<02:40, 32.03s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [14:51<02:16, 34.07s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [15:13<01:31, 30.40s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [15:37<00:57, 28.54s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [16:10<00:29, 29.80s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [16:23<00:00, 30.75s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 11000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_3.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 11000 to 11999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:27<14:13, 27.53s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [00:57<14:32, 29.08s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:19<12:32, 25.95s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [01:50<12:52, 27.60s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:18<12:36, 28.02s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [02:56<13:35, 31.36s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [03:31<13:36, 32.65s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:07<13:23, 33.50s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [04:39<12:43, 33.18s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:16<12:33, 34.23s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [05:49<11:49, 33.79s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:25<11:28, 34.43s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [07:01<11:04, 34.97s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:32<10:11, 33.99s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [08:08<09:44, 34.39s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [08:48<09:37, 36.11s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [09:22<08:50, 35.38s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [10:03<08:41, 37.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [10:44<08:18, 38.32s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [11:20<07:32, 37.70s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:54<06:41, 36.51s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [12:22<05:39, 33.91s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:58<05:12, 34.67s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [13:37<04:46, 35.84s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [14:15<04:15, 36.52s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [14:50<03:36, 36.12s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [15:20<02:51, 34.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [15:55<02:17, 34.31s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [16:27<01:41, 33.90s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [16:58<01:06, 33.03s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [17:27<00:31, 31.58s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [17:36<00:00, 33.03s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 12000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_3.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 12000 to 12999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:26<13:29, 26.11s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [00:55<14:08, 28.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:35<16:11, 33.50s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:06<15:08, 32.45s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:33<13:45, 30.59s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:02<12:55, 29.82s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [03:26<11:44, 28.18s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:03<12:22, 30.96s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [04:37<12:14, 31.91s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:11<11:52, 32.38s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [05:38<10:47, 30.83s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:11<10:26, 31.34s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [06:48<10:33, 33.34s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:16<09:28, 31.56s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [07:40<08:20, 29.42s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [08:12<07:58, 29.93s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [08:50<08:08, 32.59s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [09:26<07:50, 33.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [09:59<07:11, 33.20s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [10:40<07:06, 35.55s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:08<06:07, 33.39s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [11:40<05:29, 32.91s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:17<05:07, 34.19s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [12:51<04:33, 34.25s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [13:20<03:48, 32.66s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [13:43<02:58, 29.68s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [14:10<02:24, 28.93s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [14:50<02:08, 32.18s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [15:20<01:34, 31.65s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [15:49<01:01, 30.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [16:15<00:29, 29.23s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [16:28<00:00, 30.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 13000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_3.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 13000 to 13999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:23<12:07, 23.48s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [00:59<15:20, 30.70s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:27<14:14, 29.47s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [01:56<13:38, 29.25s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:25<13:12, 29.37s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [02:48<11:41, 26.98s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [03:15<11:17, 27.11s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [03:40<10:31, 26.31s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [04:04<09:50, 25.67s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [04:31<09:35, 26.15s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [05:04<09:52, 28.24s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [05:51<11:20, 34.05s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [06:34<11:35, 36.59s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:13<11:14, 37.46s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [07:55<10:57, 38.65s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [08:20<09:13, 34.57s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [08:51<08:21, 33.44s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [09:15<07:08, 30.60s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [09:42<06:24, 29.60s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [10:11<05:53, 29.45s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [10:35<05:07, 27.93s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [11:03<04:39, 27.95s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [11:28<04:01, 26.88s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [11:59<03:46, 28.26s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [12:42<03:49, 32.73s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [13:15<03:16, 32.82s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [13:45<02:39, 31.87s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [14:21<02:12, 33.03s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [14:51<01:36, 32.30s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [15:17<01:00, 30.37s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [15:43<00:28, 28.98s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [15:57<00:00, 29.94s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 14000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_3.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 14000 to 14999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:27<14:23, 27.86s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [00:58<14:52, 29.75s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:29<14:38, 30.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:01<14:22, 30.82s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:41<15:22, 34.17s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:07<13:32, 31.26s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [03:48<14:22, 34.49s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:21<13:40, 34.19s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [05:00<13:40, 35.67s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:33<12:41, 34.59s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [05:58<11:04, 31.67s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [06:29<10:34, 31.74s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [06:59<09:51, 31.15s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [07:34<09:39, 32.17s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [08:07<09:10, 32.39s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [08:44<08:59, 33.74s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [09:17<08:24, 33.61s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [09:50<07:49, 33.54s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [10:17<06:49, 31.49s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [10:44<06:02, 30.24s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [11:10<05:18, 28.99s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [11:42<04:59, 29.90s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [12:14<04:32, 30.31s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [12:38<03:48, 28.54s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [13:16<03:39, 31.32s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [13:51<03:15, 32.57s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [14:19<02:35, 31.14s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [14:43<01:55, 28.84s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [15:13<01:27, 29.32s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [15:44<00:59, 29.69s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [16:14<00:30, 30.04s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [16:27<00:00, 30.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 15000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_3.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 15000 to 15999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/32 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   3%|▎         | 1/32 [00:36<19:01, 36.82s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   6%|▋         | 2/32 [01:12<18:11, 36.39s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   9%|▉         | 3/32 [01:51<18:04, 37.39s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  12%|█▎        | 4/32 [02:23<16:26, 35.24s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  16%|█▌        | 5/32 [02:56<15:34, 34.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 6/32 [03:33<15:18, 35.32s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  22%|██▏       | 7/32 [04:00<13:37, 32.71s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  25%|██▌       | 8/32 [04:35<13:21, 33.38s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  28%|██▊       | 9/32 [05:15<13:33, 35.38s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  31%|███▏      | 10/32 [05:53<13:13, 36.07s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  34%|███▍      | 11/32 [06:39<13:40, 39.06s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 12/32 [07:24<13:41, 41.07s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  41%|████      | 13/32 [08:03<12:45, 40.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  44%|████▍     | 14/32 [08:43<12:04, 40.26s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  47%|████▋     | 15/32 [09:15<10:44, 37.91s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  50%|█████     | 16/32 [09:54<10:12, 38.26s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  53%|█████▎    | 17/32 [10:32<09:31, 38.07s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  56%|█████▋    | 18/32 [11:10<08:52, 38.04s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  59%|█████▉    | 19/32 [11:40<07:44, 35.76s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▎   | 20/32 [12:09<06:42, 33.51s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  66%|██████▌   | 21/32 [12:41<06:06, 33.29s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  69%|██████▉   | 22/32 [13:16<05:38, 33.80s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  72%|███████▏  | 23/32 [13:54<05:13, 34.87s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  75%|███████▌  | 24/32 [14:24<04:28, 33.60s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  78%|███████▊  | 25/32 [15:00<04:00, 34.32s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████▏ | 26/32 [15:44<03:42, 37.13s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  84%|████████▍ | 27/32 [16:23<03:07, 37.56s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  88%|████████▊ | 28/32 [17:00<02:30, 37.52s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  91%|█████████ | 29/32 [17:35<01:50, 36.69s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  94%|█████████▍| 30/32 [18:01<01:07, 33.54s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  97%|█████████▋| 31/32 [18:29<00:31, 31.72s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 32/32 [18:42<00:00, 35.07s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 16000 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_3.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n",
      "\n",
      "Processing rows 16000 to 16666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating questions:   0%|          | 0/21 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:   5%|▍         | 1/21 [00:38<12:42, 38.11s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  10%|▉         | 2/21 [01:07<10:25, 32.94s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  14%|█▍        | 3/21 [01:35<09:16, 30.92s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  19%|█▉        | 4/21 [02:09<09:06, 32.14s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  24%|██▍       | 5/21 [02:45<08:52, 33.25s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  29%|██▊       | 6/21 [03:09<07:33, 30.25s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  33%|███▎      | 7/21 [03:45<07:28, 32.01s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  38%|███▊      | 8/21 [04:11<06:33, 30.28s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  43%|████▎     | 9/21 [04:43<06:08, 30.73s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  48%|████▊     | 10/21 [05:15<05:40, 30.96s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  52%|█████▏    | 11/21 [05:56<05:41, 34.15s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  57%|█████▋    | 12/21 [06:36<05:22, 35.83s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  62%|██████▏   | 13/21 [07:05<04:30, 33.78s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  67%|██████▋   | 14/21 [07:49<04:18, 36.94s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  71%|███████▏  | 15/21 [08:25<03:39, 36.63s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  76%|███████▌  | 16/21 [09:05<03:08, 37.73s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  81%|████████  | 17/21 [09:37<02:23, 35.86s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  86%|████████▌ | 18/21 [10:07<01:42, 34.08s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  90%|█████████ | 19/21 [10:40<01:07, 33.99s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions:  95%|█████████▌| 20/21 [11:10<00:32, 32.81s/batch]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Generating questions: 100%|██████████| 21/21 [11:38<00:00, 33.26s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with 16667 rows to /content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_3.csv\n",
      "Cleared CUDA cache and ran garbage collection.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/content/drive/Shareddrives/Master_Thesis/Query_Creation/Data/df_100_chunk_3.csv\")\n",
    "\n",
    "checkpoint_path=\"/content/drive/Shareddrives/Master_Thesis/Query_Creation/Results/queries_chunk_3.csv\"\n",
    "\n",
    "final_df = run_generation_with_checkpoints(\n",
    "    df_full=df,\n",
    "    chunk_size=1000,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    user_template=USER_TEMPLATE,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    batch_size=32,\n",
    "    max_new_tokens=100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01256959d4204aa694ad6563565f89ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78300efa96a14359a60e8e33431ad497",
      "placeholder": "​",
      "style": "IPY_MODEL_763aa4b4bdb24e91b385c7337cc13254",
      "value": "tokenizer.json: 100%"
     }
    },
    "01dc4924f2f24056ac8d12f0a532af90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b16f6479a3e4513b73c18865796ff11",
      "max": 9085657,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08fbec7fc7334c58a92d4e2d2dbcd002",
      "value": 9085657
     }
    },
    "03692f67dff34cd6b0561bc8c239b78c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0436145453d84fea8c77ab478a3a6638": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_271c895fb9294a258fabbf1b4f40fe5a",
      "placeholder": "​",
      "style": "IPY_MODEL_2339a8a11fcb4f7b9f30ce1b1f9b475f",
      "value": "config.json: 100%"
     }
    },
    "05fa78cb0a98443aad1c80856a58ebc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9deace65b4994cf5941d2e9d887ac7b6",
       "IPY_MODEL_b76ab888f0854b9992c6639f3a5a5396",
       "IPY_MODEL_1713317ed98b40b2a26a2b06870a6823"
      ],
      "layout": "IPY_MODEL_c478b371e9e74b24809bb9767a2f8fbd"
     }
    },
    "06cc24f291184473b1c5cd348c765e3f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "080780fe5c9844c3b7b3ee72d27c99b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b4b50d1dc404b9e8018fe75caa7214e",
      "placeholder": "​",
      "style": "IPY_MODEL_03692f67dff34cd6b0561bc8c239b78c",
      "value": "model-00002-of-00002.safetensors: 100%"
     }
    },
    "08682b0f334547879a693ded44f5ca98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "08fbec7fc7334c58a92d4e2d2dbcd002": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0b16f6479a3e4513b73c18865796ff11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10208663f1fd416ea3424930b953da85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "13bce83f6bb04ec58fb6847294e778c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3e30394f9c3437cbdfe0c9eef770a4c",
      "max": 20919,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7bf347d995924c8598f1d6208e7c9f27",
      "value": 20919
     }
    },
    "13db93cc091b47b291ff365d84289ffc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13e913078ff2468d91ba6990e0d355b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_807de26f50e647f4b9ecae71aecfe533",
       "IPY_MODEL_cb79bfbcbf3f4f309c4a3612a0a8c83a",
       "IPY_MODEL_4de6a68e5a5644b8b054747cc011e9bc"
      ],
      "layout": "IPY_MODEL_9b5709e5356042e88311c1aec7247ef8"
     }
    },
    "151099b08b92451db5f5b4f0421b0ba7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13db93cc091b47b291ff365d84289ffc",
      "placeholder": "​",
      "style": "IPY_MODEL_dc2bd709910e4fd7bf2a24b8f382abdc",
      "value": " 20.9k/20.9k [00:00&lt;00:00, 2.21MB/s]"
     }
    },
    "16ed4704562346d09fd87eab4b6d7331": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1713317ed98b40b2a26a2b06870a6823": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b57ceafc41641f3badb13744d18dd00",
      "placeholder": "​",
      "style": "IPY_MODEL_26f092f80ab04d59bb29f902da939e91",
      "value": " 54.5k/54.5k [00:00&lt;00:00, 5.22MB/s]"
     }
    },
    "183c254716394cea99b4a6a2143c0828": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "19ad93bdb6b64750a70d1d2526ed07ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ee716d6bc9047c7b62ee249a31de4ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62830d1e720344759241874acca0c77c",
      "placeholder": "​",
      "style": "IPY_MODEL_e191c802701844ee93f64c7e46bd68c8",
      "value": " 296/296 [00:00&lt;00:00, 28.8kB/s]"
     }
    },
    "2339a8a11fcb4f7b9f30ce1b1f9b475f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "240907917f04462ba003f498dea253b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24949cd89bac41d7820b92a1ed4165be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25a06b8ffac64705aa7de34b8f5fa509": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2668f8928d704679b87c6f5c6c71be01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26f092f80ab04d59bb29f902da939e91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "271c895fb9294a258fabbf1b4f40fe5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "304ee5c19ef545c3956dc4bdabe5c333": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b1e44a793c784b0aa09a2b821cee9e48",
       "IPY_MODEL_f0b5c664e8ff4b5693bb01228949a41b",
       "IPY_MODEL_7bab1e490519459ea7156d5f31e1dee5"
      ],
      "layout": "IPY_MODEL_876a8247336549a78afea762a706934e"
     }
    },
    "370ecf5e739d46218e135f5b82f9185b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b0abc88fe2c4691b3018182942b9693": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3da10c36828c47f5a9a2366bb661d8aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f012533869242848ec700aa82b2bade": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df7a2152d6524f06a763bb7d4e06a0b5",
      "placeholder": "​",
      "style": "IPY_MODEL_9ecb8fc01cae4284a15ca09e1d66b16a",
      "value": " 878/878 [00:00&lt;00:00, 93.2kB/s]"
     }
    },
    "3fd5c8afe3cd4a80811f9a6a70b71165": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_406c64ff8d184541821a52a395d71e10",
      "max": 878,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aeaf5da7b5a44c759fe78b68e84267e2",
      "value": 878
     }
    },
    "406c64ff8d184541821a52a395d71e10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "421e3caabc764fe7b1aba7aa1a8214fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4965798a3b47410eaea6c4e7d1afe58b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b21238a28a8c4454895aadb41f6afa13",
      "placeholder": "​",
      "style": "IPY_MODEL_863e80b8a58a4fe8a250a7f067b68653",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "4b57ceafc41641f3badb13744d18dd00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4de6a68e5a5644b8b054747cc011e9bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_370ecf5e739d46218e135f5b82f9185b",
      "placeholder": "​",
      "style": "IPY_MODEL_d20aeb3714b34f13a7bf5be040147e3c",
      "value": " 4.97G/4.97G [00:58&lt;00:00, 109MB/s]"
     }
    },
    "586d81b8ee414bc785c132a7d868f3b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1ccb64dab1743b39b45b87872b53ab1",
      "placeholder": "​",
      "style": "IPY_MODEL_c5e2cb8ed8b649bba15fba0535ccb04a",
      "value": "model.safetensors.index.json: 100%"
     }
    },
    "5d1ba4df4b1d439c9e9f6057c9b9f81e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9dc60374b0fa4c2b919f04b446bb5bf8",
      "placeholder": "​",
      "style": "IPY_MODEL_88443a36ec0f454683c83a2457ae9c5f",
      "value": " 9.09M/9.09M [00:00&lt;00:00, 20.9MB/s]"
     }
    },
    "62830d1e720344759241874acca0c77c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "651628f1d89747c4a7ec12dd03532ab0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68bb8b5cb85347fb87023194fe7b48ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_080780fe5c9844c3b7b3ee72d27c99b0",
       "IPY_MODEL_6ea2c474d9ec4a21a7e8ecfe12e6d029",
       "IPY_MODEL_85fad10a986d48768ace00d56c51dbbd"
      ],
      "layout": "IPY_MODEL_8d9439de20e442b98a9a9612a48da4b1"
     }
    },
    "69a4d8b45cbd4ae0854af1a8bf57b9f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_586d81b8ee414bc785c132a7d868f3b8",
       "IPY_MODEL_13bce83f6bb04ec58fb6847294e778c2",
       "IPY_MODEL_151099b08b92451db5f5b4f0421b0ba7"
      ],
      "layout": "IPY_MODEL_25a06b8ffac64705aa7de34b8f5fa509"
     }
    },
    "6ba40d8009204247802b18e1932ecb34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9dab50cf91b6430bbd91d5a75fe67276",
       "IPY_MODEL_d71cf890e08e4e9f878d541d0dd2915d",
       "IPY_MODEL_be70cc2d2de24babaf6186ffe5728621"
      ],
      "layout": "IPY_MODEL_86c9c0f2f035448791f2dff7fd57f422"
     }
    },
    "6cec9a6e14584db7a7fe8b276567f583": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6ea2c474d9ec4a21a7e8ecfe12e6d029": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6a47d9c8f4f495daa708edff8569e6a",
      "max": 1459729952,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fe48f9cc48424a299cb5cfbe611b134d",
      "value": 1459729952
     }
    },
    "763aa4b4bdb24e91b385c7337cc13254": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78300efa96a14359a60e8e33431ad497": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b4b50d1dc404b9e8018fe75caa7214e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b928b1c31b64574967f2c5cfe60f32d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb5575ca514d4b67839a4e7d26684148",
      "placeholder": "​",
      "style": "IPY_MODEL_c046d2703ae8445fa541c451c87db82c",
      "value": "Fetching 2 files: 100%"
     }
    },
    "7bab1e490519459ea7156d5f31e1dee5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b43d7d5946f4f87b418edd8abcc828b",
      "placeholder": "​",
      "style": "IPY_MODEL_24949cd89bac41d7820b92a1ed4165be",
      "value": " 2/2 [00:31&lt;00:00, 14.39s/it]"
     }
    },
    "7bf347d995924c8598f1d6208e7c9f27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7f3e43d244ad4f378424dcc3e5097150": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "807de26f50e647f4b9ecae71aecfe533": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_240907917f04462ba003f498dea253b8",
      "placeholder": "​",
      "style": "IPY_MODEL_8ed698c3a96a42028b701772852357bb",
      "value": "model-00001-of-00002.safetensors: 100%"
     }
    },
    "85fad10a986d48768ace00d56c51dbbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d58a7c9a4a024088afb68902a605e97d",
      "placeholder": "​",
      "style": "IPY_MODEL_e03928de7486475b8694091542585a7b",
      "value": " 1.46G/1.46G [00:26&lt;00:00, 116MB/s]"
     }
    },
    "863e80b8a58a4fe8a250a7f067b68653": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86c9c0f2f035448791f2dff7fd57f422": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "876a8247336549a78afea762a706934e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88443a36ec0f454683c83a2457ae9c5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b43d7d5946f4f87b418edd8abcc828b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d9439de20e442b98a9a9612a48da4b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ed698c3a96a42028b701772852357bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94ef933e70314522be743caf5750e8bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b5709e5356042e88311c1aec7247ef8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cfb00c5b77c49bfacb067010948313f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9dab50cf91b6430bbd91d5a75fe67276": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94ef933e70314522be743caf5750e8bd",
      "placeholder": "​",
      "style": "IPY_MODEL_fb24fa107d3d4e1fa797168cea403c96",
      "value": "generation_config.json: 100%"
     }
    },
    "9dc60374b0fa4c2b919f04b446bb5bf8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9deace65b4994cf5941d2e9d887ac7b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f69f9c3106fe4ae3bd9e726aadbc2aa1",
      "placeholder": "​",
      "style": "IPY_MODEL_cad02e8b364d4dc2976952f9217e9e08",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "9ecb8fc01cae4284a15ca09e1d66b16a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a35bfe2d33c7470ea7d68b578dbb2668": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a4f91ef0974c4d3aab3a14fe9bbac7e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_01256959d4204aa694ad6563565f89ef",
       "IPY_MODEL_01dc4924f2f24056ac8d12f0a532af90",
       "IPY_MODEL_5d1ba4df4b1d439c9e9f6057c9b9f81e"
      ],
      "layout": "IPY_MODEL_a61d8829dadd4d9eb7378fb048a67f45"
     }
    },
    "a61d8829dadd4d9eb7378fb048a67f45": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6a47d9c8f4f495daa708edff8569e6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7ba096b1c3e4315908c2e6e443f809b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aeaf5da7b5a44c759fe78b68e84267e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b1e44a793c784b0aa09a2b821cee9e48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b92cd839e7d84b7fbcc3c38eed60f4e6",
      "placeholder": "​",
      "style": "IPY_MODEL_08682b0f334547879a693ded44f5ca98",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "b21238a28a8c4454895aadb41f6afa13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3e30394f9c3437cbdfe0c9eef770a4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b76ab888f0854b9992c6639f3a5a5396": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9cfb00c5b77c49bfacb067010948313f",
      "max": 54528,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6cec9a6e14584db7a7fe8b276567f583",
      "value": 54528
     }
    },
    "b92cd839e7d84b7fbcc3c38eed60f4e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be70cc2d2de24babaf6186ffe5728621": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da5fbd68f4b34ca3a7c82bba8cf0474f",
      "placeholder": "​",
      "style": "IPY_MODEL_3b0abc88fe2c4691b3018182942b9693",
      "value": " 189/189 [00:00&lt;00:00, 10.3kB/s]"
     }
    },
    "c046d2703ae8445fa541c451c87db82c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c478b371e9e74b24809bb9767a2f8fbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5e2cb8ed8b649bba15fba0535ccb04a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cad02e8b364d4dc2976952f9217e9e08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb79bfbcbf3f4f309c4a3612a0a8c83a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_421e3caabc764fe7b1aba7aa1a8214fa",
      "max": 4965799096,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a35bfe2d33c7470ea7d68b578dbb2668",
      "value": 4965799096
     }
    },
    "cfc36161e7e347cb801a8596d55460ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d80cdebafb0c404eb23cfb4c686b5e4d",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_10208663f1fd416ea3424930b953da85",
      "value": 2
     }
    },
    "d0999f87ba8948fd9ac777c5abf33739": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5d8c708723d4f718342b66b1fe34f7e",
      "placeholder": "​",
      "style": "IPY_MODEL_16ed4704562346d09fd87eab4b6d7331",
      "value": " 2/2 [00:58&lt;00:00, 58.71s/it]"
     }
    },
    "d1ccb64dab1743b39b45b87872b53ab1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d20aeb3714b34f13a7bf5be040147e3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d58a7c9a4a024088afb68902a605e97d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d71cf890e08e4e9f878d541d0dd2915d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06cc24f291184473b1c5cd348c765e3f",
      "max": 189,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ddf093f2590947e1b6749455126b0a4e",
      "value": 189
     }
    },
    "d80cdebafb0c404eb23cfb4c686b5e4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da5fbd68f4b34ca3a7c82bba8cf0474f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbe97c0a959544a9b3e4042968404c92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19ad93bdb6b64750a70d1d2526ed07ca",
      "max": 296,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a7ba096b1c3e4315908c2e6e443f809b",
      "value": 296
     }
    },
    "dc2bd709910e4fd7bf2a24b8f382abdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ddf093f2590947e1b6749455126b0a4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df7a2152d6524f06a763bb7d4e06a0b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e03928de7486475b8694091542585a7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e191c802701844ee93f64c7e46bd68c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec37de5ae58f4ac5a5db4e3d2d5953c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b928b1c31b64574967f2c5cfe60f32d",
       "IPY_MODEL_cfc36161e7e347cb801a8596d55460ba",
       "IPY_MODEL_d0999f87ba8948fd9ac777c5abf33739"
      ],
      "layout": "IPY_MODEL_2668f8928d704679b87c6f5c6c71be01"
     }
    },
    "f0b5c664e8ff4b5693bb01228949a41b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3da10c36828c47f5a9a2366bb661d8aa",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_183c254716394cea99b4a6a2143c0828",
      "value": 2
     }
    },
    "f5d8c708723d4f718342b66b1fe34f7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f69f9c3106fe4ae3bd9e726aadbc2aa1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb24fa107d3d4e1fa797168cea403c96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb5575ca514d4b67839a4e7d26684148": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe48f9cc48424a299cb5cfbe611b134d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ffc868d75b974af6b0ab9e8508107419": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0436145453d84fea8c77ab478a3a6638",
       "IPY_MODEL_3fd5c8afe3cd4a80811f9a6a70b71165",
       "IPY_MODEL_3f012533869242848ec700aa82b2bade"
      ],
      "layout": "IPY_MODEL_7f3e43d244ad4f378424dcc3e5097150"
     }
    },
    "fff0824b66db422886b8c6d0614dc03a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4965798a3b47410eaea6c4e7d1afe58b",
       "IPY_MODEL_dbe97c0a959544a9b3e4042968404c92",
       "IPY_MODEL_1ee716d6bc9047c7b62ee249a31de4ee"
      ],
      "layout": "IPY_MODEL_651628f1d89747c4a7ec12dd03532ab0"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
